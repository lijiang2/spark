/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.execution.streaming.state

import java.util.{Timer, TimerTask}

import scala.collection.mutable
import scala.util.control.NonFatal

import org.apache.hadoop.conf.Configuration

import org.apache.spark.{SparkConf, SparkEnv}
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.expressions.UnsafeRow
import org.apache.spark.sql.types.StructType


/** Unique identifier for a [[StateStore]] */
case class StateStoreId(rootLocation: String, operatorId: Long, partitionId: Int)

/**
 * Base trait for a versioned key-value store used for streaming aggregations
 */
trait StateStore {

  /** Unique identifier of the store */
  def id: StateStoreId

  /** Version of the data in this store before committing updates. */
  def version: Long

  /**
   * Update the value of a key using the value generated by the update function.
   * This can be called only after prepareForUpdates() has been called in the same thread.
   */
  def update(key: UnsafeRow, updateFunc: Option[UnsafeRow] => UnsafeRow): Unit

  /**
   * Remove keys that match the following condition.
   * This can be called only after prepareForUpdates() has been called in the current thread.
   */
  def remove(condition: UnsafeRow => Boolean): Unit

  /**
   * Commit all the updates that have been made to the store.
   * This can be called only after prepareForUpdates() has been called in the current thread.
   */
  def commit(): Long

  /** Cancel all the updates that have been made to the store. */
  def cancel(): Unit

  /**
   * Iterator of store data after a set of updates have been committed.
   * This can be called only after commitUpdates() has been called in the current thread.
   */
  def iterator(): Iterator[(UnsafeRow, UnsafeRow)]

  /**
   * Iterator of the updates that have been committed.
   * This can be called only after commitUpdates() has been called in the current thread.
   */
  def updates(): Iterator[StoreUpdate]

  /**
   * Whether all updates have been committed
   */
  def hasCommitted: Boolean
}


trait StateStoreProvider {

  /** Get the store with the existing version. */
  def getStore(version: Long): StateStore

  /** Optional method for providers to allow for background management */
  def doMaintenance(): Unit = { }
}

sealed trait StoreUpdate
case class ValueAdded(key: UnsafeRow, value: UnsafeRow) extends StoreUpdate
case class ValueUpdated(key: UnsafeRow, value: UnsafeRow) extends StoreUpdate
case class KeyRemoved(key: UnsafeRow) extends StoreUpdate


/**
 * Companion object to [[StateStore]] that provides helper methods to create and retrieve stores
 * by their unique ids. In addition, when a SparkContext is active (i.e. SparkEnv.get is not null),
 * it also runs a periodic background tasks to do maintenance on the loaded stores. For each
 * store, tt uses the [[StateStoreCoordinator]] to ensure whether the current loaded instance of
 * the store is the active instance. Accordingly, it either keeps it loaded and performance
 * maintenance, or unloads the store.
 */
private[state] object StateStore extends Logging {

  val MAINTENANCE_INTERVAL_CONFIG = "spark.sql.streaming.stateStore.maintenanceInterval"
  val MAINTENANCE_INTERVAL_DEFAULT_SECS = 60

  private val loadedProviders = new mutable.HashMap[StateStoreId, StateStoreProvider]()
  private val managementTimer = new Timer("StateStore Timer", true)

  @volatile private var managementTask: TimerTask = null
  @volatile private var _coordRef: StateStoreCoordinatorRef = null

  /** Get or create a store associated with the id. */
  def get(
    storeId: StateStoreId,
    keySchema: StructType,
    valueSchema: StructType,
    version: Long,
    hadoopConf: Configuration
  ): StateStore = {
    require(version >= 0)
    val storeProvider = loadedProviders.synchronized {
      startMaintenanceIfNeeded()
      val sparkConf = Option(SparkEnv.get).map(_.conf).getOrElse(new SparkConf)
      val provider = loadedProviders.getOrElseUpdate(
        storeId,
        new HDFSBackedStateStoreProvider(storeId, keySchema, valueSchema, sparkConf, hadoopConf))
      reportActiveStoreInstance(storeId)
      provider
    }
    storeProvider.getStore(version)
  }

  def remove(storeId: StateStoreId): Unit = loadedProviders.synchronized {
    loadedProviders.remove(storeId)
  }

  def isLoaded(storeId: StateStoreId): Boolean = loadedProviders.synchronized {
    loadedProviders.contains(storeId)
  }

  /** Unload and stop all state store provider */
  def stop(): Unit = loadedProviders.synchronized {
    loadedProviders.clear()
    _coordRef = null
    if (managementTask != null) {
      managementTask.cancel()
      managementTask = null
    }
    logInfo("StateStore stopped")
  }

  /** Start the periodic maintenance task if not already started and if Spark active */
  private def startMaintenanceIfNeeded(): Unit = loadedProviders.synchronized {
    val env = SparkEnv.get
    if (managementTask == null && env != null) {
      managementTask = new TimerTask {
        override def run(): Unit = { doMaintenance() }
      }
      val periodMs = env.conf.getTimeAsMs(
        MAINTENANCE_INTERVAL_CONFIG,
        s"${MAINTENANCE_INTERVAL_DEFAULT_SECS}s")
      managementTimer.schedule(managementTask, periodMs, periodMs)
      logInfo("StateStore maintenance timer started")
    }
  }

  /**
   * Execute background management task in all the loaded store providers if they are still
   * the active instances according to the coordinator.
   */
  private def doMaintenance(): Unit = {
    logDebug("Doing maintenance")
    loadedProviders.synchronized { loadedProviders.toSeq }.foreach { case (id, provider) =>
      try {
        if (verifyIfStoreInstanceActive(id)) {
          provider.doMaintenance()
        } else {
          remove(id)
          logInfo(s"Unloaded $provider")
        }
      } catch {
        case NonFatal(e) =>
          logWarning(s"Error managing $provider")
      }
    }
  }

  private def reportActiveStoreInstance(storeId: StateStoreId): Unit = {
    try {
      val host = SparkEnv.get.blockManager.blockManagerId.host
      val executorId = SparkEnv.get.blockManager.blockManagerId.executorId
      coordinatorRef.foreach(_.reportActiveInstance(storeId, host, executorId))
      logDebug(s"Reported that the loaded instance $storeId is active")
    } catch {
      case NonFatal(e) =>
        logWarning(s"Error reporting active instance of $storeId")
    }
  }

  private def verifyIfStoreInstanceActive(storeId: StateStoreId): Boolean = {
    try {
      val executorId = SparkEnv.get.blockManager.blockManagerId.executorId
      val verified =
        coordinatorRef.map(_.verifyIfInstanceActive(storeId, executorId)).getOrElse(false)
      logDebug(s"Verifyied whether the loaded instance $storeId is active: $verified" )
      verified
    } catch {
      case NonFatal(e) =>
        logWarning(s"Error verifying active instance of $storeId")
        false
    }
  }

  private def coordinatorRef: Option[StateStoreCoordinatorRef] = synchronized {
    val env = SparkEnv.get
    if (env != null) {
      if (_coordRef == null) {
        _coordRef = StateStoreCoordinatorRef(env)
      }
      logDebug(s"Retrieved reference to StateStoreCoordinator: ${_coordRef}")
      Some(_coordRef)
    } else {
      _coordRef = null
      None
    }
  }
}

